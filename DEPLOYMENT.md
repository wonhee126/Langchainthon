"""
MLX ëª¨ë¸ API ì„œë²„
ë¡œì»¬ì—ì„œ ì‹¤í–‰í•˜ì—¬ í´ë¼ìš°ë“œ ì•±ì— LLM ê¸°ëŠ¥ ì œê³µ
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict
import uvicorn
from mlx_lm import load, generate
from contextlib import asynccontextmanager
import os

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥
model = None
tokenizer = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global model, tokenizer
    
    print("ğŸ¤– MLX ëª¨ë¸ ë¡œë”©...")
    model_name = "mlx-community/Qwen2.5-7B-Instruct-4bit"
    model, tokenizer = load(model_name)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    yield
    
    # ì •ë¦¬ ì‘ì—…
    print("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ")


app = FastAPI(
    title="Dream Bot MLX API",
    description="ê¿ˆí•´ëª½ ìƒë‹´ê°€ LLM API",
    lifespan=lifespan
)


class GenerateRequest(BaseModel):
    """ìƒì„± ìš”ì²­ ëª¨ë¸"""
    prompt: str
    max_tokens: int = 300
    temperature: float = 0.7
    top_p: float = 0.9


class GenerateResponse(BaseModel):
    """ìƒì„± ì‘ë‹µ ëª¨ë¸"""
    text: str
    tokens_generated: int


@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "model": "Qwen2.5-7B-Instruct-4bit"}


@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """í…ìŠ¤íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # MLX ìƒì„±
        response = generate(
            model,
            tokenizer,
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temp=request.temperature,
            top_p=request.top_p,
        )
        
        # í† í° ìˆ˜ ê³„ì‚°
        tokens = tokenizer.encode(response)
        
        return GenerateResponse(
            text=response,
            tokens_generated=len(tokens)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat")
async def chat_completion(messages: List[Dict[str, str]], max_tokens: int = 300):
    """ChatGPT ìŠ¤íƒ€ì¼ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ìƒì„±
        response = generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temp=0.7,
            top_p=0.9,
        )
        
        return {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": response
                }
            }]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ë¡œ í¬íŠ¸ ì„¤ì • ê°€ëŠ¥
    port = int(os.getenv("MLX_SERVER_PORT", "8000"))
    
    print(f"ğŸš€ MLX API ì„œë²„ ì‹œì‘: http://localhost:{port}")
    print("ğŸ“ API ë¬¸ì„œ: http://localhost:{port}/docs")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    ) 