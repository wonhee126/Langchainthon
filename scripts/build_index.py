#!/usr/bin/env python
"""
Reddit ìƒë‹´ì‚¬ RAG ì¸ë±ìŠ¤ ë¹Œë”
AITA JSON íŒŒì¼ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
"""

import os
import json
import pickle
import gc
from pathlib import Path
from typing import List, Dict

import faiss
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer  # ì‹¤ì œ ì„ë² ë”© í™œì„±í™”!
from langchain.text_splitter import RecursiveCharacterTextSplitter
import torch


class RedditIndexBuilder:
    """Reddit ìƒë‹´ì‚¬ ì¸ë±ìŠ¤ ë¹Œë”"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.embedding_model = "intfloat/multilingual-e5-small"
        self.embedding_dim = 384
        self.index_dir = Path("index")
        self.data_dir = Path("data")
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (Reddit í¬ìŠ¤íŠ¸ì— ë§ê²Œ ì¡°ì •)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Reddit í¬ìŠ¤íŠ¸ëŠ” ë³´í†µ ê¸¸ì–´ì„œ í¬ê¸° ì¦ê°€
            chunk_overlap=100,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.index_dir.mkdir(exist_ok=True)
        
    def infer_verdict(self, comments: List[Dict], top_k: int = 5) -> str:
        """ëŒ“ê¸€ì—ì„œ AITA íŒì • ì¶”ì¶œ"""
        votes = {"YTA": 0, "NTA": 0, "ESH": 0, "NAH": 0, "INFO": 0}
        
        if not comments:
            return "UNKNOWN"
        
        # ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ ëŒ“ê¸€ë“¤
        top_comments = sorted(comments, key=lambda x: x.get('score', 0), reverse=True)[:top_k]
        
        for comment in top_comments:
            message = comment.get('message', '').upper().strip()
            # ê° ì•½ì–´ê°€ ëŒ“ê¸€ ì‹œì‘ ë¶€ë¶„ì— ìˆëŠ”ì§€ í™•ì¸
            for abbr in votes:
                if message.startswith(abbr):
                    votes[abbr] += 1
                    break
        
        # ê°€ì¥ ë§ì€ í‘œë¥¼ ë°›ì€ íŒì • ë°˜í™˜
        max_count = max(votes.values())
        if max_count > 0:
            return max(votes, key=votes.get)
        return "UNKNOWN"

    def load_json_data(self, filename: str, source_name: str) -> List[Dict]:
        """JSON íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬ (AITA íŒì • ì¶”ì¶œ í¬í•¨)"""
        print(f"ğŸ“š {source_name} JSON íŒŒì¼ ë¡œë”© ì¤‘...")
        
        documents = []
        json_file = self.data_dir / filename
        
        if not json_file.exists():
            print(f"âš ï¸ {json_file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return documents
        
        # AITAëŠ” JSON ë°°ì—´ í˜•íƒœ, TIFUëŠ” ì¤„ ë‹¨ìœ„ JSON
        if source_name == "AITA":
            # JSON ë°°ì—´ ì „ì²´ë¥¼ í•œ ë²ˆì— ë¡œë“œ
            with open(json_file, 'r', encoding='utf-8') as f:
                posts = json.load(f)
            
            print(f"ğŸ“„ {len(posts)}ê°œì˜ AITA í¬ìŠ¤íŠ¸ ë¡œë“œë¨")
            
            for post_num, post in enumerate(tqdm(posts, desc=f"{source_name} ë°ì´í„° ì²˜ë¦¬")):
                try:
                    
                    # ë°ì´í„° ì†ŒìŠ¤ë³„ í•„ë“œ ë§¤í•‘
                    if source_name == "TIFU":
                        title = post.get('trimmed_title', post.get('title', ''))
                        content = post.get('selftext_without_tldr', post.get('selftext', ''))
                        tldr = post.get('tldr', '')
                        
                        # ì „ì²´ í…ìŠ¤íŠ¸ êµ¬ì„±
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}"
                        if tldr:
                            full_text += f"\n\nìš”ì•½: {tldr}"
                            
                    elif source_name == "AITA":
                        # ìƒˆë¡œìš´ AITA ë°ì´í„° êµ¬ì¡°: submission ê°ì²´ ì•ˆì— ëª¨ë“  ì •ë³´
                        if 'submission' in post:
                            submission = post['submission']
                            title = submission.get('title', '')
                            content = submission.get('selftext', '')
                            score = submission.get('score', 0)
                            submission_id = submission.get('submission_id', '')
                            comments = post.get('comments', [])
                            permalink = submission.get('permalink', '')
                        else:
                            # ê¸°ì¡´ êµ¬ì¡° ëŒ€ì‘
                            title = post.get('title', '')
                            content = post.get('selftext', '')
                            score = post.get('score', 0)
                            submission_id = post.get('submission_id', post.get('id', ''))
                            comments = post.get('comments', [])
                            permalink = post.get('permalink', '')
                        
                        # ì „ì²´ URL ìƒì„±
                        if permalink:
                            full_url = f"https://reddit.com{permalink}" if not permalink.startswith("http") else permalink
                        else:
                            full_url = f"https://reddit.com/r/AmItheAsshole/comments/{submission_id}"
                        
                        # AITA íŒì • ì¶”ì¶œ
                        verdict = self.infer_verdict(comments)
                        
                        # AITAëŠ” ë³´í†µ ì œëª©ì— "AITA for..." í˜•íƒœ
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}" if title else content
                    
                    else:
                        # ê¸°ë³¸ ì²˜ë¦¬ (ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤ ëŒ€ì‘)
                        title = post.get('title', '')
                        content = post.get('selftext', post.get('content', post.get('text', '')))
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}" if title else content
                    
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                    if not content or len(full_text.strip()) < 100:
                        continue
                    
                    # ì²­í¬ë¡œ ë¶„í• 
                    chunks = self.text_splitter.split_text(full_text)
                    
                    for i, chunk in enumerate(chunks):
                        if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                            documents.append({
                                'text': chunk.strip(),
                                'metadata': {
                                    'source': source_name,
                                    'post_id': submission_id if source_name == "AITA" else post.get('id', post.get('submission_id', f'{source_name.lower()}_{line_num}')),
                                    'title': title,
                                    'chunk_id': i,
                                    'total_chunks': len(chunks),
                                    'score': score if source_name == "AITA" else post.get('score', 0),
                                    'num_comments': post.get('num_comments', 0),
                                    'comments': post.get('comments', []),
                                    'verdict': verdict if source_name == "AITA" else "N/A",
                                    'url': full_url
                                }
                            })
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬ (1000ê°œì”© ì²˜ë¦¬ í›„ ì •ë¦¬)
                    if post_num % 1000 == 0 and post_num > 0:
                        gc.collect()
                        
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"í¬ìŠ¤íŠ¸ {post_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
                    
        else:
            # TIFUëŠ” ì¤„ ë‹¨ìœ„ JSONìœ¼ë¡œ ì²˜ë¦¬
            with open(json_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(tqdm(f, desc=f"{source_name} ë°ì´í„° ë¡œë”©")):
                    try:
                        post = json.loads(line.strip())
                        
                        # TIFU ë°ì´í„° ì²˜ë¦¬
                        title = post.get('trimmed_title', post.get('title', ''))
                        content = post.get('selftext_without_tldr', post.get('selftext', ''))
                        tldr = post.get('tldr', '')
                        
                        # ì „ì²´ í…ìŠ¤íŠ¸ êµ¬ì„±
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}"
                        if tldr:
                            full_text += f"\n\nìš”ì•½: {tldr}"
                        
                        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                        if not content or len(full_text.strip()) < 100:
                            continue
                        
                        # ì²­í¬ë¡œ ë¶„í• 
                        chunks = self.text_splitter.split_text(full_text)
                        
                        for i, chunk in enumerate(chunks):
                            if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                                documents.append({
                                    'text': chunk.strip(),
                                    'metadata': {
                                        'source': source_name,
                                        'post_id': post.get('id', post.get('submission_id', f'{source_name.lower()}_{line_num}')),
                                        'title': title,
                                        'chunk_id': i,
                                        'total_chunks': len(chunks),
                                        'score': post.get('score', 0),
                                        'num_comments': post.get('num_comments', 0)
                                    }
                                })
                        
                        # ë©”ëª¨ë¦¬ ê´€ë¦¬ (1000ê°œì”© ì²˜ë¦¬ í›„ ì •ë¦¬)
                        if line_num % 1000 == 0 and line_num > 0:
                            gc.collect()
                            
                    except (json.JSONDecodeError, KeyError) as e:
                        print(f"ë¼ì¸ {line_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
        
        print(f"âœ… {source_name}ì—ì„œ {len(documents)}ê°œ ì²­í¬ ìƒì„±")
        return documents
    
    def create_embeddings(self, documents: List[Dict]) -> np.ndarray:
        """ë¬¸ì„œ ì²­í¬ë“¤ì˜ ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”© ìƒì„± (í¬ë¡œìŠ¤ í”Œë«í¼)"""
        print("ğŸ§  ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # í”Œë«í¼ë³„ ìµœì í™” ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
        import platform
        
        # MPS (Apple Silicon GPU) ì‚¬ìš© ì„¤ì •
        if torch.backends.mps.is_available() and torch.backends.mps.is_built() and platform.system() == "Darwin":
            device = "mps"
            print("ğŸš€ Apple Silicon MPS ê°€ì† í™œì„±í™”!")
        else:
            device = "cpu"
            print("ğŸ–¥ï¸ CPU ì‚¬ìš© (MPS ì‚¬ìš© ë¶ˆê°€)")
            
        print(f"ğŸ“¥ {self.embedding_model} ëª¨ë¸ ë¡œë”© (ë””ë°”ì´ìŠ¤: {device})...")
        embedder = SentenceTransformer(self.embedding_model, device=device)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [doc['text'] for doc in documents]
        print(f"ğŸ“ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹œì‘...")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
        # MPS/GPU í™˜ê²½ì—ì„œëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ íš¨ìœ¨ì 
        batch_size = 128
        print(f"âš¡ ë°°ì¹˜ í¬ê¸°: {batch_size} (ì†ë„ ìµœì í™”)")
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ì˜ë¯¸ë¡ ì  ì„ë² ë”© ìƒì„±"):
            batch_texts = texts[i:i+batch_size]
            
            # ì„ë² ë”© ìƒì„± (ì •ê·œí™” ìë™ ì ìš©)
            batch_embeddings = embedder.encode(
                batch_texts,
                normalize_embeddings=True,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            all_embeddings.append(batch_embeddings)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % (batch_size * 10) == 0:
                gc.collect()
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        final_embeddings = np.vstack(all_embeddings).astype('float32')
        print(f"âœ… ì‹¤ì œ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {final_embeddings.shape}")
        print("ğŸ¯ ì´ì œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        
        return final_embeddings
    
    def build_faiss_index(self, embeddings: np.ndarray):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ—ï¸ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # ì°¨ì› ìˆ˜
        dimension = embeddings.shape[1]
        
        # IndexFlatIP ì‚¬ìš© (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        index = faiss.IndexFlatIP(dimension)
        
        # ì„ë² ë”© ì¶”ê°€
        index.add(embeddings.astype('float32'))
        
        print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    def save_index(self, index, documents: List[Dict], embeddings: np.ndarray):
        """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        print("ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(index, str(self.index_dir / "reddit_index.faiss"))
        
        # ë¬¸ì„œ ì²­í¬ ì €ì¥
        with open(self.index_dir / "chunks.pkl", "wb") as f:
            pickle.dump(documents, f)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            'embedding_model': self.embedding_model,
            'total_chunks': len(documents),
            'embedding_dimension': embeddings.shape[1],
            'index_type': 'FlatIP',
            'chunk_size': 800,
            'chunk_overlap': 100,
            'data_sources': ['AITA'],
            'data_format': 'JSON',
            'note': 'Using semantic embeddings with multilingual-e5-base model',
            'verdict_enabled': True,
            'verdict_types': ['YTA', 'NTA', 'ESH', 'NAH', 'INFO', 'UNKNOWN']
        }
        
        with open(self.index_dir / "config.json", "w", encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_dir}")
    
    def build(self):
        """ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸ¤– Reddit ìƒë‹´ì‚¬ RAG ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘ (AITA ì „ìš© ë²„ì „)")
        print("=" * 60)
        
        try:
            # JSON íŒŒì¼ë“¤ ë¡œë”©
            documents = []
            
            # TIFU ë°ì´í„° ë¡œë“œ ì œê±°
            # tifu_docs = self.load_json_data("tifu_all_tokenized_and_filtered.json", "TIFU")
            # documents.extend(tifu_docs)
            
            # AITA ë°ì´í„°ë§Œ ë¡œë“œ
            aita_docs = self.load_json_data("aita_data.json", "AITA")
            documents.extend(aita_docs)
            
            if not documents:
                raise ValueError("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            print(f"ğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
            
            # ì„ë² ë”© ìƒì„± (ì„ì‹œ ëœë¤)
            embeddings = self.create_embeddings(documents)
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            index = self.build_faiss_index(embeddings)
            
            # ì €ì¥
            self.save_index(index, documents, embeddings)
            
            print("=" * 60)
            print("ğŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ!")
            print(f"  - ì´ ì²­í¬ ìˆ˜: {len(documents)}")
            print(f"  - ì„ë² ë”© ì°¨ì›: {embeddings.shape[1]}")
            print(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {index.ntotal}")
            print()
            print("ğŸ¯ ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
            print("   multilingual-e5-base ëª¨ë¸ë¡œ í•œêµ­ì–´-ì˜ì–´ í¬ë¡œìŠ¤ë§êµ¬ì–¼ ê²€ìƒ‰ ì§€ì›")
            print()
            print("ğŸ“ ë°ì´í„° íŒŒì¼ ì •ë³´:")
            print("   - AITA: aita_data.json âœ…")
            
        except Exception as e:
            print(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    builder = RedditIndexBuilder()
    builder.build()


if __name__ == "__main__":
    main() 