"""
PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° FAISS ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
M2 MacBook Air 8GB RAMì— ìµœì í™”
"""

import os
import gc
import json
import pickle
import psutil
from pathlib import Path
from typing import List, Dict, Tuple

import faiss
import numpy as np
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm


class PDFIndexBuilder:
    """PDF ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, embedding_model_name: str = "intfloat/multilingual-e5-base"):
        """
        ì´ˆê¸°í™”
        
        Args:
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„ (e5-baseëŠ” í•œêµ­ì–´ ì§€ì›)
        """
        print(f"ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {embedding_model_name}")
        # MPS í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ CPU ì‚¬ìš©
        self.embedder = SentenceTransformer(embedding_model_name, device="cpu")
        
        # M2 Mac ìµœì í™”: ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
        self.embedder.max_seq_length = 512  # ë©”ëª¨ë¦¬ ì ˆì•½
        self.batch_size = 8  # 8GB RAMì— ì í•©
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=200,  # ì•½ 200 í† í°
            chunk_overlap=50,  # ì¤‘ë³µìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´
            separators=["\n\n", "\n", ".", "ã€‚", "!", "?", ";", ":", " ", ""],
            length_function=len,
        )
        
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, Dict]:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (ì „ì²´ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)
        """
        print(f"ğŸ“„ PDF ì½ëŠ” ì¤‘: {pdf_path.name}")
        
        reader = PdfReader(pdf_path)
        text_parts = []
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í˜ì´ì§€ë³„ ì²˜ë¦¬
        for i, page in enumerate(tqdm(reader.pages, desc="í˜ì´ì§€ ì¶”ì¶œ")):
            try:
                text = page.extract_text()
                if text.strip():
                    text_parts.append(text)
                    
                # 10í˜ì´ì§€ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i % 10 == 0:
                    gc.collect()
                    
            except Exception as e:
                print(f"âš ï¸ í˜ì´ì§€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                continue
        
        full_text = "\n\n".join(text_parts)
        
        metadata = {
            "source": pdf_path.name,
            "pages": len(reader.pages),
            "characters": len(full_text)
        }
        
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(full_text):,} ë¬¸ì")
        
        return full_text, metadata
    
    def create_chunks(self, text: str, source: str) -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            text: ì „ì²´ í…ìŠ¤íŠ¸
            source: ì†ŒìŠ¤ íŒŒì¼ëª…
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ (í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„°)
        """
        print(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        
        chunks = self.text_splitter.split_text(text)
        
        # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        chunk_docs = []
        for i, chunk in enumerate(chunks):
            if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
                chunk_docs.append({
                    "text": chunk,
                    "metadata": {
                        "source": source,
                        "chunk_id": i,
                        "chunk_size": len(chunk)
                    }
                })
        
        print(f"âœ… {len(chunk_docs)}ê°œ ì²­í¬ ìƒì„±")
        
        return chunk_docs
    
    def create_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë°°ì—´
        """
        embeddings = []
        
        # ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in tqdm(range(0, len(texts), self.batch_size), desc="ì„ë² ë”© ìƒì„±"):
            batch = texts[i:i + self.batch_size]
            
            # ì„ë² ë”© ìƒì„±
            batch_embeddings = self.embedder.encode(
                batch,
                normalize_embeddings=True,  # ì •ê·œí™”ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”
                show_progress_bar=False
            )
            
            embeddings.extend(batch_embeddings)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % (self.batch_size * 10) == 0:
                gc.collect()
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                memory_percent = psutil.virtual_memory().percent
                if memory_percent > 80:
                    print(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {memory_percent:.1f}%")
        
        return np.array(embeddings, dtype='float32')
    
    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """
        FAISS ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            embeddings: ì„ë² ë”© ë²¡í„°
            
        Returns:
            FAISS ì¸ë±ìŠ¤
        """
        print(f"ğŸ”¨ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        dimension = embeddings.shape[1]
        
        # M2 Mac ìµœì í™”: ê°„ë‹¨í•œ ì¸ë±ìŠ¤ ì‚¬ìš©
        # IndexFlatL2ëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¦„
        index = faiss.IndexFlatL2(dimension)
        
        # ì •ê·œí™”ëœ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ IP (ë‚´ì ) ì¸ë±ìŠ¤ë„ ê°€ëŠ¥
        # index = faiss.IndexFlatIP(dimension)
        
        index.add(embeddings)
        
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        
        return index
    
    def process_pdfs(self, pdf_dir: Path, output_dir: Path):
        """
        PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            pdf_dir: PDF íŒŒì¼ ë””ë ‰í† ë¦¬
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir.mkdir(exist_ok=True)
        
        # PDF íŒŒì¼ ëª©ë¡
        pdf_files = list(pdf_dir.glob("*.pdf"))
        print(f"ğŸ“š {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
        
        all_chunks = []
        
        # ê° PDF ì²˜ë¦¬
        for pdf_path in pdf_files:
            try:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text, metadata = self.extract_text_from_pdf(pdf_path)
                
                # ì²­í¬ ìƒì„±
                chunks = self.create_chunks(text, pdf_path.name)
                all_chunks.extend(chunks)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                
            except Exception as e:
                print(f"âŒ {pdf_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ“Š ì „ì²´ ì²­í¬ ìˆ˜: {len(all_chunks)}")
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        texts = [chunk["text"] for chunk in all_chunks]
        
        # ì„ë² ë”© ìƒì„±
        print("\nğŸ§  ì„ë² ë”© ìƒì„± ì‹œì‘...")
        embeddings = self.create_embeddings_batch(texts)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        index = self.build_faiss_index(embeddings)
        
        # ì €ì¥
        print("\nğŸ’¾ ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(index, str(output_dir / "dream_index.faiss"))
        
        # ì²­í¬ ì •ë³´ ì €ì¥ (í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„°)
        with open(output_dir / "chunks.pkl", "wb") as f:
            pickle.dump(all_chunks, f)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            "embedding_model": "intfloat/multilingual-e5-base",
            "chunk_size": 200,
            "chunk_overlap": 50,
            "total_chunks": len(all_chunks),
            "dimension": embeddings.shape[1],
            "pdf_files": [pdf.name for pdf in pdf_files]
        }
        
        with open(output_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print("\nâœ… ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    pdf_dir = Path("data")
    output_dir = Path("index")
    
    # ì¸ë±ìŠ¤ ë¹Œë” ìƒì„± ë° ì‹¤í–‰
    builder = PDFIndexBuilder()
    builder.process_pdfs(pdf_dir, output_dir)
    
    # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
    memory = psutil.virtual_memory()
    print(f"\nğŸ’» ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory.percent:.1f}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")


if __name__ == "__main__":
    main() 