#!/usr/bin/env python
"""
ê¿ˆí•´ëª½ RAG ì¸ë±ìŠ¤ ë¹Œë”
PDF ë¬¸ì„œë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
"""

import os
import json
import pickle
import gc
from pathlib import Path
from typing import List, Dict

import faiss
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader


class DreamIndexBuilder:
    """ê¿ˆí•´ëª½ ë¬¸ì„œ ì¸ë±ìŠ¤ ë¹Œë”"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.embedding_model = "intfloat/multilingual-e5-base"
        self.index_dir = Path("index")
        self.data_dir = Path("data")
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (ë” í° ì²­í¬ë¡œ ì„¤ì •)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=100,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.index_dir.mkdir(exist_ok=True)
        
    def load_pdfs(self) -> List[Dict]:
        """PDF íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• """
        print("ğŸ“š PDF íŒŒì¼ ë¡œë”© ì¤‘...")
        
        documents = []
        pdf_files = list(self.data_dir.glob("*.pdf"))
        
        if not pdf_files:
            raise FileNotFoundError(f"{self.data_dir}ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        for pdf_path in tqdm(pdf_files, desc="PDF ë¡œë”©"):
            print(f"  - {pdf_path.name} ì²˜ë¦¬ ì¤‘...")
            
            # PDF ë¡œë” ì‚¬ìš©
            loader = PyPDFLoader(str(pdf_path))
            pages = loader.load()
            
            # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            full_text = "\n".join([page.page_content for page in pages])
            
            # ì²­í¬ë¡œ ë¶„í• 
            chunks = self.text_splitter.split_text(full_text)
            
            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) > 100:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                    documents.append({
                        'text': chunk.strip(),
                        'metadata': {
                            'source': pdf_path.name,
                            'chunk_id': i,
                            'total_chunks': len(chunks)
                        }
                    })
        
        print(f"âœ… ì´ {len(documents)}ê°œ ì²­í¬ ìƒì„±")
        return documents
    
    def create_embeddings(self, documents: List[Dict]) -> np.ndarray:
        """ë¬¸ì„œ ì²­í¬ë“¤ì˜ ì„ë² ë”© ìƒì„±"""
        print("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        
        # CPU ì‚¬ìš© ê°•ì œ
        embedder = SentenceTransformer(
            self.embedding_model, 
            device="cpu"
        )
        embedder.max_seq_length = 512
        
        print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
        texts = [doc['text'] for doc in documents]
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
        batch_size = 16
        embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ì„ë² ë”© ìƒì„±"):
            batch = texts[i:i + batch_size]
            batch_embeddings = embedder.encode(
                batch,
                normalize_embeddings=True,
                show_progress_bar=False
            )
            embeddings.append(batch_embeddings)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % (batch_size * 10) == 0:
                gc.collect()
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        all_embeddings = np.vstack(embeddings)
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {all_embeddings.shape}")
        
        return all_embeddings
    
    def build_faiss_index(self, embeddings: np.ndarray):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ—ï¸ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # ì°¨ì› ìˆ˜
        dimension = embeddings.shape[1]
        
        # IndexFlatIP ì‚¬ìš© (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        index = faiss.IndexFlatIP(dimension)
        
        # ì„ë² ë”© ì¶”ê°€
        index.add(embeddings.astype('float32'))
        
        print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    def save_index(self, index, documents: List[Dict], embeddings: np.ndarray):
        """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        print("ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(index, str(self.index_dir / "dream_index.faiss"))
        
        # ë¬¸ì„œ ì²­í¬ ì €ì¥
        with open(self.index_dir / "chunks.pkl", "wb") as f:
            pickle.dump(documents, f)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            'embedding_model': self.embedding_model,
            'total_chunks': len(documents),
            'embedding_dimension': embeddings.shape[1],
            'index_type': 'FlatIP',
            'chunk_size': 300,
            'chunk_overlap': 50
        }
        
        with open(self.index_dir / "config.json", "w", encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_dir}")
    
    def build(self):
        """ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸŒ™ ê¿ˆí•´ëª½ RAG ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘")
        print("=" * 50)
        
        try:
            # 1. PDF ë¡œë”© ë° ì²­í¬ ë¶„í• 
            documents = self.load_pdfs()
            
            # 2. ì„ë² ë”© ìƒì„±
            embeddings = self.create_embeddings(documents)
            
            # 3. FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            index = self.build_faiss_index(embeddings)
            
            # 4. ì €ì¥
            self.save_index(index, documents, embeddings)
            
            print("=" * 50)
            print("ğŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ!")
            print(f"ğŸ“Š í†µê³„:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {len(set(doc['metadata']['source'] for doc in documents))}")
            print(f"  - ì´ ì²­í¬ ìˆ˜: {len(documents)}")
            print(f"  - ì„ë² ë”© ì°¨ì›: {embeddings.shape[1]}")
            print(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {index.ntotal}")
            
        except Exception as e:
            print(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    builder = DreamIndexBuilder()
    builder.build()


if __name__ == "__main__":
    main() 