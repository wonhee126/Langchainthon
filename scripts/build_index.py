#!/usr/bin/env python
"""
Reddit ìƒë‹´ì‚¬ RAG ì¸ë±ìŠ¤ ë¹Œë”
TIFUì™€ AITA JSON íŒŒì¼ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
"""

import os
import json
import pickle
import gc
from pathlib import Path
from typing import List, Dict

import faiss
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer  # ì‹¤ì œ ì„ë² ë”© í™œì„±í™”!
from langchain.text_splitter import RecursiveCharacterTextSplitter


class RedditIndexBuilder:
    """Reddit ìƒë‹´ì‚¬ ì¸ë±ìŠ¤ ë¹Œë”"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.embedding_model = "intfloat/multilingual-e5-base"  # ì„¤ì •ìš©
        self.embedding_dim = 768  # ì„ì‹œ ì°¨ì› ì„¤ì •
        self.index_dir = Path("index")
        self.data_dir = Path("data")
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (Reddit í¬ìŠ¤íŠ¸ì— ë§ê²Œ ì¡°ì •)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Reddit í¬ìŠ¤íŠ¸ëŠ” ë³´í†µ ê¸¸ì–´ì„œ í¬ê¸° ì¦ê°€
            chunk_overlap=100,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.index_dir.mkdir(exist_ok=True)
        
    def load_json_data(self, filename: str, source_name: str) -> List[Dict]:
        """JSON íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬ (TIFUì™€ AITA í†µí•© ì²˜ë¦¬)"""
        print(f"ğŸ“š {source_name} JSON íŒŒì¼ ë¡œë”© ì¤‘...")
        
        documents = []
        json_file = self.data_dir / filename
        
        if not json_file.exists():
            print(f"âš ï¸ {json_file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return documents
        
        # JSON íŒŒì¼ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        with open(json_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(tqdm(f, desc=f"{source_name} ë°ì´í„° ë¡œë”©")):
                try:
                    post = json.loads(line.strip())
                    
                    # ë°ì´í„° ì†ŒìŠ¤ë³„ í•„ë“œ ë§¤í•‘
                    if source_name == "TIFU":
                        title = post.get('trimmed_title', post.get('title', ''))
                        content = post.get('selftext_without_tldr', post.get('selftext', ''))
                        tldr = post.get('tldr', '')
                        
                        # ì „ì²´ í…ìŠ¤íŠ¸ êµ¬ì„±
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}"
                        if tldr:
                            full_text += f"\n\nìš”ì•½: {tldr}"
                            
                    elif source_name == "AITA":
                        title = post.get('title', '')
                        content = post.get('selftext', '')
                        
                        # AITAëŠ” ë³´í†µ ì œëª©ì— "AITA for..." í˜•íƒœ
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}" if title else content
                    
                    else:
                        # ê¸°ë³¸ ì²˜ë¦¬ (ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤ ëŒ€ì‘)
                        title = post.get('title', '')
                        content = post.get('selftext', post.get('content', post.get('text', '')))
                        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}" if title else content
                    
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                    if not content or len(full_text.strip()) < 100:
                        continue
                    
                    # ì²­í¬ë¡œ ë¶„í• 
                    chunks = self.text_splitter.split_text(full_text)
                    
                    for i, chunk in enumerate(chunks):
                        if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                            documents.append({
                                'text': chunk.strip(),
                                'metadata': {
                                    'source': source_name,
                                    'post_id': post.get('id', post.get('submission_id', f'{source_name.lower()}_{line_num}')),
                                    'title': title,
                                    'chunk_id': i,
                                    'total_chunks': len(chunks),
                                    'score': post.get('score', 0),
                                    'num_comments': post.get('num_comments', 0)
                                }
                            })
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬ (1000ê°œì”© ì²˜ë¦¬ í›„ ì •ë¦¬)
                    if line_num % 1000 == 0 and line_num > 0:
                        gc.collect()
                        
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"ë¼ì¸ {line_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        print(f"âœ… {source_name}ì—ì„œ {len(documents)}ê°œ ì²­í¬ ìƒì„±")
        return documents
    
    def create_embeddings(self, documents: List[Dict]) -> np.ndarray:
        """ë¬¸ì„œ ì²­í¬ë“¤ì˜ ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”© ìƒì„± (í¬ë¡œìŠ¤ í”Œë«í¼)"""
        print("ğŸ§  ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # í”Œë«í¼ë³„ ìµœì í™” (ì„ íƒì  ì ìš©)
        import platform
        
        if platform.system() == "Darwin" and platform.machine() == "arm64":
            print("ğŸ Apple Silicon Mac ìµœì í™” ì ìš©")
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        elif platform.system() == "Windows":
            print("ğŸªŸ Windows í™˜ê²½ ìµœì í™” ì ìš©")
        else:
            print("ğŸ–¥ï¸ ë²”ìš© í™˜ê²½ ì„¤ì •")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ“¥ {self.embedding_model} ëª¨ë¸ ë¡œë”©...")
        embedder = SentenceTransformer(self.embedding_model, device="cpu")
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [doc['text'] for doc in documents]
        print(f"ğŸ“ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹œì‘...")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
        batch_size = 32  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ì˜ë¯¸ë¡ ì  ì„ë² ë”© ìƒì„±"):
            batch_texts = texts[i:i+batch_size]
            
            # ì„ë² ë”© ìƒì„± (ì •ê·œí™” ìë™ ì ìš©)
            batch_embeddings = embedder.encode(
                batch_texts,
                normalize_embeddings=True,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            all_embeddings.append(batch_embeddings)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % (batch_size * 10) == 0:
                gc.collect()
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        final_embeddings = np.vstack(all_embeddings).astype('float32')
        print(f"âœ… ì‹¤ì œ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {final_embeddings.shape}")
        print("ğŸ¯ ì´ì œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        
        return final_embeddings
    
    def build_faiss_index(self, embeddings: np.ndarray):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ—ï¸ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # ì°¨ì› ìˆ˜
        dimension = embeddings.shape[1]
        
        # IndexFlatIP ì‚¬ìš© (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        index = faiss.IndexFlatIP(dimension)
        
        # ì„ë² ë”© ì¶”ê°€
        index.add(embeddings.astype('float32'))
        
        print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    def save_index(self, index, documents: List[Dict], embeddings: np.ndarray):
        """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        print("ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(index, str(self.index_dir / "reddit_index.faiss"))
        
        # ë¬¸ì„œ ì²­í¬ ì €ì¥
        with open(self.index_dir / "chunks.pkl", "wb") as f:
            pickle.dump(documents, f)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            'embedding_model': self.embedding_model,
            'total_chunks': len(documents),
            'embedding_dimension': embeddings.shape[1],
            'index_type': 'FlatIP',
            'chunk_size': 800,
            'chunk_overlap': 100,
            'data_sources': ['TIFU'],
            'data_format': 'JSON',
            'note': 'Using semantic embeddings with multilingual-e5-base model'
        }
        
        with open(self.index_dir / "config.json", "w", encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_dir}")
    
    def build(self):
        """ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸ¤– Reddit ìƒë‹´ì‚¬ RAG ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘ (JSON í†µí•© ë²„ì „)")
        print("=" * 60)
        
        try:
            # JSON íŒŒì¼ë“¤ ë¡œë”©
            documents = []
            
            # TIFU ë°ì´í„° ë¡œë“œ
            tifu_docs = self.load_json_data("tifu_all_tokenized_and_filtered.json", "TIFU")
            documents.extend(tifu_docs)
            
            if not documents:
                raise ValueError("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            print(f"ğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
            
            # ì„ë² ë”© ìƒì„± (ì„ì‹œ ëœë¤)
            embeddings = self.create_embeddings(documents)
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            index = self.build_faiss_index(embeddings)
            
            # ì €ì¥
            self.save_index(index, documents, embeddings)
            
            print("=" * 60)
            print("ğŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ!")
            print(f"  - ì´ ì²­í¬ ìˆ˜: {len(documents)}")
            print(f"  - ì„ë² ë”© ì°¨ì›: {embeddings.shape[1]}")
            print(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {index.ntotal}")
            print()
            print("ğŸ¯ ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
            print("   multilingual-e5-base ëª¨ë¸ë¡œ í•œêµ­ì–´-ì˜ì–´ í¬ë¡œìŠ¤ë§êµ¬ì–¼ ê²€ìƒ‰ ì§€ì›")
            print()
            print("ğŸ“ ë°ì´í„° íŒŒì¼ ì •ë³´:")
            print("   - TIFU: tifu_all_tokenized_and_filtered.json âœ…")
            
        except Exception as e:
            print(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    builder = RedditIndexBuilder()
    builder.build()


if __name__ == "__main__":
    main() 