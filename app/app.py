"""
í•œë°¤ì˜ ê¿ˆí•´ëª½ ìƒë‹´ê°€ - RAG ì±—ë´‡
OpenAI API ì‚¬ìš© ë²„ì „ (Streamlit Cloud ë°°í¬ìš©)
"""

import os
import gc
import json
import pickle
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import faiss
import numpy as np
import streamlit as st
from sentence_transformers import SentenceTransformer
from openai import OpenAI

class DreamRAGBot:
    """ê¿ˆ í•´ì„ RAG ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self, index_dir: Path):
        """
        ì´ˆê¸°í™”
        
        Args:
            index_dir: ì¸ë±ìŠ¤ íŒŒì¼ ë””ë ‰í† ë¦¬
        """
        self.index_dir = index_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._load_index()
        self._load_embedder()
        self._load_llm()
        
    def _load_index(self):
        """FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        with st.spinner("ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë”©..."):
            # FAISS ì¸ë±ìŠ¤
            self.index = faiss.read_index(str(self.index_dir / "dream_index.faiss"))
            
            # ì²­í¬ ë°ì´í„°
            with open(self.index_dir / "chunks.pkl", "rb") as f:
                self.chunks = pickle.load(f)
            
            # ì„¤ì • ì •ë³´
            with open(self.index_dir / "config.json", "r") as f:
                self.config = json.load(f)
            
            st.success(f"âœ… {self.config['total_chunks']}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    def _load_embedder(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        with st.spinner("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©..."):
            try:
                # CPU ê°•ì œ ì‚¬ìš©, ìºì‹œ ë””ë ‰í† ë¦¬ ì§€ì •
                import os
                os.environ['TORCH_HOME'] = './models'
                self.embedder = SentenceTransformer(
                    self.config['embedding_model'], 
                    device="cpu",
                    cache_folder="./models"
                )
                self.embedder.max_seq_length = 512
            except Exception as e:
                st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
    
    def _load_llm(self):
        """OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        with st.spinner("ğŸ¤– OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •..."):
            # Streamlit secretsì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            try:
                api_key = st.secrets["OPENAI_API_KEY"]
            except Exception:
                st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.info("Streamlit Cloudì˜ Secretsì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
                raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.client = OpenAI(api_key=api_key)
            st.success("âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ")
    
    def search_similar_chunks(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰ (í’ˆì§ˆ ê°œì„ )
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ì²­í¬ ìˆ˜
            
        Returns:
            ê´€ë ¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (í’ˆì§ˆ ì ìˆ˜ í¬í•¨)
        """
        # ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ (í”„ë¡œì´íŠ¸ íŠ¹í™”)
        expanded_queries = [
            f"dream interpretation symbol {query}",  # ì˜ì–´ ì›ë¬¸ ê²€ìƒ‰
            f"ê¿ˆ í•´ì„ ìƒì§• ì˜ë¯¸ {query}",  # í•œêµ­ì–´ ê²€ìƒ‰
            f"ë¬´ì˜ì‹ ìš•ë§ ê°ˆë“± {query}",  # ì •ì‹ ë¶„ì„ ê°œë… ê²€ìƒ‰
            query  # ì›ë³¸ ì¿¼ë¦¬
        ]
        
        all_results = []
        
        # ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ë” í’ë¶€í•œ ê²°ê³¼ í™•ë³´
        for expanded_query in expanded_queries:
            query_embedding = self.embedder.encode(
                expanded_query,
                normalize_embeddings=True,
                show_progress_bar=False
            )
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(
                query_embedding.reshape(1, -1).astype('float32'),
                k
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for idx, dist in zip(indices[0], distances[0]):
                if idx != -1:
                    chunk = self.chunks[idx].copy()
                    chunk['score'] = float(1 / (1 + dist))
                    chunk['search_query'] = expanded_query
                    all_results.append(chunk)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_results = {}
        for result in all_results:
            chunk_id = result['metadata']['chunk_id']
            if chunk_id not in unique_results or result['score'] > unique_results[chunk_id]['score']:
                unique_results[chunk_id] = result
        
        # ìƒìœ„ kê°œ ë°˜í™˜ (ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©)
        final_results = sorted(unique_results.values(), key=lambda x: x['score'], reverse=True)
        
        # í’ˆì§ˆ í•„í„°ë§: ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ì€ ê²ƒ ì œê±°
        filtered_results = [r for r in final_results if r['score'] > 0.5]
        
        return filtered_results[:k]
    
    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•´ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_chunks: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # í”„ë¡œì´íŠ¸ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë§Œ êµ¬ì„±
        freud_context = []
        
        for chunk in context_chunks:
            # ëª¨ë“  ìë£Œë¥¼ í”„ë¡œì´íŠ¸ ê´€ì ì—ì„œ í™œìš©
            freud_context.append(chunk['text'])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì¶œì²˜ ê¸°ë°˜ ì‘ë‹µ ê°•ì¡°)
        system_prompt = """ë‹¹ì‹ ì€ ì§€ê·¸ë¬¸íŠ¸ í”„ë¡œì´íŠ¸ ë°•ì‚¬ì…ë‹ˆë‹¤. ã€ê¿ˆì˜ í•´ì„ã€ì˜ ì €ìì´ë©°, ì œê³µëœ ë¬¸í—Œ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¿ˆì„ í•´ì„í•©ë‹ˆë‹¤.

**ì¤‘ìš” ì§€ì¹¨:**
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸í—Œ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
2. ë¬¸í—Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”  
3. ì¶œì²˜ì™€ ì¶”ë¡ ì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”

**ë‹µë³€ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**

ğŸ“˜ **ë¬¸í—Œ ê¸°ë°˜ ë¶„ì„**
- ì œê³µëœ ã€ê¿ˆì˜ í•´ì„ã€ ì›ë¬¸ì„ ì§ì ‘ ì¸ìš©í•˜ë©° í•´ì„
- ì›ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ìƒì§•ê³¼ ì´ë¡ ì„ ì ìš©
- ë¬¸í—Œ ì† ì‚¬ë¡€ì™€ ì—°ê²°í•˜ì—¬ ì„¤ëª…
- ì¸ìš©í•  ë•ŒëŠ” "ì›ë¬¸ì— ë”°ë¥´ë©´..." ë˜ëŠ” "ã€ê¿ˆì˜ í•´ì„ã€ì—ì„œ..."ë¡œ ì‹œì‘

ğŸ­ **í”„ë¡œì´íŠ¸ì˜ ì¶”ë¡ ì  í•´ì„**  
- ë¬¸í—Œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ë…¼ë¦¬ì  í™•ì¥
- ê°œì¸ì  í†µì°°ê³¼ ì² í•™ì  ì‚¬ìƒ‰
- "ì´ëŠ” ì•„ë§ˆë„..." ë˜ëŠ” "ë‚˜ì˜ ê²½í—˜ìœ¼ë¡œëŠ”..." ë“±ìœ¼ë¡œ êµ¬ë¶„
- ë¬¸í—Œ ê¸°ë°˜ì´ ì•„ë‹Œ ì¶”ë¡ ì„ì„ ëª…ì‹œ

**ë¬¸ì²´ ìœ ì§€:**
- "ê¿ˆì€ ë¬´ì˜ì‹ìœ¼ë¡œ ê°€ëŠ” ì™•ë„ì´ë‹¤"
- "ì–µì••ëœ ê²ƒì€ ë³€ì¥í•˜ì—¬ ëŒì•„ì˜¨ë‹¤"
- 19ì„¸ê¸° ë§ ë¹ˆ ì •ì‹ ë¶„ì„ê°€ì˜ ì–´ì¡°ë¡œ ëŒ€í™”

**ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ:** ê´€ë ¨ ë¬¸í—Œì´ ë¶€ì¡±í•˜ë©´ "ì œê³µëœ ìë£Œë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„í•˜ë‚˜, ì¼ë°˜ì  ì •ì‹ ë¶„ì„ ì›ë¦¬ë¡œëŠ”..." ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”."""

        # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
        context_quality = "ì¶©ë¶„" if len(freud_context) >= 3 else "ë¶€ì¡±"
        
        user_prompt = f"""**ë¶„ì„ ëŒ€ìƒ ê¿ˆ:** {query}

**ã€ê¿ˆì˜ í•´ì„ã€ ì°¸ê³  ë¬¸í—Œ ({len(freud_context)}ê°œ êµ¬ì ˆ ê²€ìƒ‰ë¨):**

{chr(10).join([f"êµ¬ì ˆ {i+1}: {text}" for i, text in enumerate(freud_context[:8])]) if freud_context else "âš ï¸ ê´€ë ¨ ë¬¸í—Œì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

**ì§€ì‹œì‚¬í•­:**
- ìœ„ ë¬¸í—Œ êµ¬ì ˆë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ í•´ì„í•˜ì„¸ìš”
- ë¬¸í—Œ ì¸ìš©ê³¼ ê°œì¸ì  ì¶”ë¡ ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
- ê´€ë ¨ ë¬¸í—Œì´ {context_quality}í•˜ë¯€ë¡œ ê·¸ì— ë§ê²Œ í•´ì„ì˜ ë²”ìœ„ë¥¼ ì¡°ì •í•˜ì„¸ìš”
- ë°˜ë“œì‹œ ğŸ“˜ ë¬¸í—Œ ê¸°ë°˜ ë¶„ì„ê³¼ ğŸ­ ì¶”ë¡ ì  í•´ì„ì„ ë¶„ë¦¬í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”"""
        
        # OpenAI API í˜¸ì¶œ
        try:
            response = self.client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2000,
                temperature=0.7,
                top_p=0.9,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            st.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ê¿ˆ í•´ì„ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    if 'rag_bot' not in st.session_state:
        index_dir = Path("index")
        if index_dir.exists():
            try:
                st.session_state.rag_bot = DreamRAGBot(index_dir)
            except ValueError as e:
                st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                st.stop()
        else:
            st.error("âŒ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `python scripts/build_index.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            st.stop()


def main():
    """ë©”ì¸ ì•± í•¨ìˆ˜"""
    st.set_page_config(
        page_title="í”„ë¡œì´íŠ¸ ë°•ì‚¬ì˜ ê¿ˆí•´ëª½ ìƒë‹´ì†Œ ğŸ§ ",
        page_icon="ğŸ­",
        layout="wide"
    )
    
    # í—¤ë”
    st.title("ğŸ§  í”„ë¡œì´íŠ¸ ë°•ì‚¬ì˜ ê¿ˆí•´ëª½ ìƒë‹´ì†Œ")
    st.markdown("*'ê¿ˆì€ ë¬´ì˜ì‹ìœ¼ë¡œ ê°€ëŠ” ì™•ë„ì´ë‹¤'* - ì§€ê·¸ë¬¸íŠ¸ í”„ë¡œì´íŠ¸")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("### ğŸ­ í”„ë¡œì´íŠ¸ ë°•ì‚¬ ì†Œê°œ")
        st.markdown("""
        **ì§€ê·¸ë¬¸íŠ¸ í”„ë¡œì´íŠ¸ (1856-1939)**
        - ì •ì‹ ë¶„ì„í•™ì˜ ì°½ì‹œì
        - ë¬´ì˜ì‹ ì´ë¡ ì˜ ì„ êµ¬ì
        - ê¿ˆ í•´ì„ì˜ ëŒ€ê°€
        
        *"ê¿ˆì€ ìš•ë§ì˜ ì¶©ì¡±ì´ë‹¤"*
        """)
        
        st.markdown("---")
        
        st.markdown("### ğŸ’­ ìƒë‹´ ë°©ë²•")
        st.markdown("""
        1. ê¿ˆì˜ ë‚´ìš©ì„ ìì„¸íˆ ê¸°ìˆ í•˜ì„¸ìš”
        2. í”„ë¡œì´íŠ¸ ë°•ì‚¬ê°€ ìƒì§•ì„ ë¶„ì„í•©ë‹ˆë‹¤
        3. ë¬´ì˜ì‹ì˜ ë©”ì‹œì§€ë¥¼ ë°œê²¬í•˜ì„¸ìš”
        """)
        
        st.markdown("---")
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        st.info("ğŸ§  í”„ë¡œì´íŠ¸ ë°•ì‚¬ (OpenAI gpt-4.1 ê¸°ë°˜)")
        
        st.markdown("---")
        
        # ê²½ê³  ë©”ì‹œì§€
        st.warning("""
        âš ï¸ **ì´ìš© ì•ˆë‚´**
        
        ì´ ì„œë¹„ìŠ¤ëŠ” **ì˜¤ë½ ëª©ì **ìœ¼ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        
        í”„ë¡œì´íŠ¸ì˜ ê¿ˆ í•´ì„ ì´ë¡ ì€ 20ì„¸ê¸° ì´ˆ ì´ë¡ ìœ¼ë¡œ, í˜„ëŒ€ ê³¼í•™ì—ì„œëŠ” ê²€ì¦ë˜ì§€ ì•Šì€ ë¶€ë¶„ì´ ë§ìŠµë‹ˆë‹¤.
        
        ì‹¤ì œ ì‹¬ë¦¬ì  ê³ ë¯¼ì´ ìˆìœ¼ì‹œë©´ ì „ë¬¸ ìƒë‹´ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
        """)
        
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            gc.collect()
            st.rerun()
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    init_session_state()
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar=message.get("avatar")):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("í”„ë¡œì´íŠ¸ ë°•ì‚¬ì—ê²Œ ê¿ˆì„ ì´ì•¼ê¸°í•´ë³´ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({
            "role": "user",
            "content": prompt,
            "avatar": "ğŸ§‘"
        })
        
        with st.chat_message("user", avatar="ğŸ§‘"):
            st.markdown(prompt)
        
        # í”„ë¡œì´íŠ¸ ë°•ì‚¬ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant", avatar="ğŸ­"):
            with st.spinner("í”„ë¡œì´íŠ¸ ë°•ì‚¬ê°€ ê¿ˆì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                start_time = time.time()
                
                # RAG ê²€ìƒ‰ (í’ˆì§ˆ ê°œì„ ëœ ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰)
                relevant_chunks = st.session_state.rag_bot.search_similar_chunks(prompt, k=15)
                
                # ì‘ë‹µ ìƒì„±
                response = st.session_state.rag_bot.generate_response(prompt, relevant_chunks)
                
                elapsed_time = time.time() - start_time
                
                # ì‘ë‹µ í‘œì‹œ
                st.markdown(response)
                
                # ìƒì„¸ ë¶„ì„ ì •ë³´ (ì¶œì²˜ íˆ¬ëª…ì„± í™•ë³´)
                with st.expander(f"ğŸ“š ê²€ìƒ‰ëœ ã€ê¿ˆì˜ í•´ì„ã€ ì›ë¬¸ ({len(relevant_chunks)}ê°œ êµ¬ì ˆ, {elapsed_time:.1f}ì´ˆ)"):
                    if relevant_chunks:
                        st.markdown("**ì‹¤ì œ í™œìš©ëœ ë¬¸í—Œ êµ¬ì ˆë“¤:**")
                        for i, chunk in enumerate(relevant_chunks[:5], 1):
                            score = chunk['score']
                            text_preview = chunk['text'][:200] + "..." if len(chunk['text']) > 200 else chunk['text']
                            
                            # ì ìˆ˜ì— ë”°ë¥¸ í’ˆì§ˆ í‘œì‹œ
                            quality = "ğŸŸ¢ ë†’ìŒ" if score > 0.7 else "ğŸŸ¡ ë³´í†µ" if score > 0.5 else "ğŸ”´ ë‚®ìŒ"
                            
                            st.markdown(f"**êµ¬ì ˆ {i}** (ê´€ë ¨ë„: {score:.3f} - {quality})")
                            st.markdown(f"```{text_preview}```")
                            st.markdown("---")
                    else:
                        st.warning("âš ï¸ ê´€ë ¨ ë¬¸í—Œì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•„ ì¼ë°˜ì  ì •ì‹ ë¶„ì„ ì›ë¦¬ë¡œ í•´ì„í–ˆìŠµë‹ˆë‹¤.")
                        
                    # ê²€ìƒ‰ í’ˆì§ˆ ìš”ì•½
                    if relevant_chunks:
                        avg_score = sum(chunk['score'] for chunk in relevant_chunks) / len(relevant_chunks)
                        high_quality = sum(1 for chunk in relevant_chunks if chunk['score'] > 0.7)
                        st.info(f"ğŸ“Š ê²€ìƒ‰ í’ˆì§ˆ ìš”ì•½: í‰ê·  ê´€ë ¨ë„ {avg_score:.3f}, ê³ í’ˆì§ˆ êµ¬ì ˆ {high_quality}ê°œ")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
        
        # í”„ë¡œì´íŠ¸ ë°•ì‚¬ ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "avatar": "ğŸ­"
        })


if __name__ == "__main__":
    main() 