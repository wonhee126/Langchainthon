"""
í•œë°¤ì˜ ê¿ˆí•´ëª½ ìƒë‹´ê°€ - RAG ì±—ë´‡
M2 MacBook Air 8GB RAM ìµœì í™” ë²„ì „
"""

import os
import gc
import json
import pickle
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import faiss
import numpy as np
import psutil
import streamlit as st
from sentence_transformers import SentenceTransformer
from mlx_lm import load, generate
import mlx.core as mx


class DreamRAGBot:
    """ê¿ˆ í•´ì„ RAG ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self, index_dir: Path):
        """
        ì´ˆê¸°í™”
        
        Args:
            index_dir: ì¸ë±ìŠ¤ íŒŒì¼ ë””ë ‰í† ë¦¬
        """
        self.index_dir = index_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._load_index()
        self._load_embedder()
        self._load_llm()
        
    def _load_index(self):
        """FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        with st.spinner("ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë”©..."):
            # FAISS ì¸ë±ìŠ¤
            self.index = faiss.read_index(str(self.index_dir / "dream_index.faiss"))
            
            # ì²­í¬ ë°ì´í„°
            with open(self.index_dir / "chunks.pkl", "rb") as f:
                self.chunks = pickle.load(f)
            
            # ì„¤ì • ì •ë³´
            with open(self.index_dir / "config.json", "r") as f:
                self.config = json.load(f)
            
            st.success(f"âœ… {self.config['total_chunks']}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    def _load_embedder(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        with st.spinner("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©..."):
            self.embedder = SentenceTransformer(self.config['embedding_model'])
            self.embedder.max_seq_length = 512
    
    def _load_llm(self):
        """MLX ê¸°ë°˜ Qwen ëª¨ë¸ ë¡œë“œ"""
        with st.spinner("ğŸ¤– Qwen ëª¨ë¸ ë¡œë”© (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ)..."):
            # Qwen 2.5-7B Int4 ëª¨ë¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            model_name = "mlx-community/Qwen2.5-7B-Instruct-4bit"
            
            try:
                self.model, self.tokenizer = load(model_name)
                st.success("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                st.info("ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´: `mlx-lm download mlx-community/Qwen2.5-7B-Instruct-4bit`")
                raise
    
    def search_similar_chunks(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ì²­í¬ ìˆ˜
            
        Returns:
            ê´€ë ¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedder.encode(
            query,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1).astype('float32'),
            k
        )
        
        # ê²°ê³¼ ì¶”ì¶œ
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤
                chunk = self.chunks[idx].copy()
                chunk['score'] = float(1 / (1 + dist))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                results.append(chunk)
        
        return results
    
    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:
        """
        LLMì„ ì‚¬ìš©í•´ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_chunks: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        freud_context = []
        who_context = []
        
        for chunk in context_chunks:
            if "freud" in chunk['metadata']['source'].lower():
                freud_context.append(chunk['text'])
            else:
                who_context.append(chunk['text'])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """ë‹¹ì‹ ì€ ê¿ˆ í•´ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
í”„ë¡œì´íŠ¸ì˜ ì •ì‹ ë¶„ì„í•™ì  ê´€ì ê³¼ WHOì˜ í˜„ëŒ€ ìˆ˜ë©´ê³¼í•™ì„ í†µí•©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 3ë‹¨ê³„ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:

â‘  <ê³ ì „ì  í•´ì„>: í”„ë¡œì´íŠ¸ ì´ë¡  ê¸°ë°˜ (ì•½ê°„ ì—„ìˆ™í•˜ê³  í•™ìˆ ì  í†¤)
â‘¡ <í˜„ëŒ€ ê³¼í•™>: WHO ìˆ˜ë©´ ê°€ì´ë“œ ê¸°ë°˜ (ê°„ê²°í•˜ê³  ì‹¤ìš©ì )  
â‘¢ <í†µí•© ì¡°ì–¸>: ë‘ ê´€ì ì„ ê²°í•©í•œ ì‹¤ì²œì  ì¡°ì–¸ (ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤)

ê° ë‹¨ê³„ëŠ” 2-3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""

        user_prompt = f"""ê¿ˆ ë‚´ìš©: {query}

í”„ë¡œì´íŠ¸ ìë£Œ:
{' '.join(freud_context[:2]) if freud_context else 'ê´€ë ¨ ìë£Œ ì—†ìŒ'}

WHO ìˆ˜ë©´ ìë£Œ:
{' '.join(who_context[:2]) if who_context else 'ê´€ë ¨ ìë£Œ ì—†ìŒ'}

ìœ„ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ 3ë‹¨ê³„ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        # í† í° ì œí•œ (M2 8GB ìµœì í™”)
        max_tokens = 300
        
        # ìƒì„±
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # MLX ìƒì„±
        response = generate(
            self.model,
            self.tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=0.7,  # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
            top_p=0.9,
        )
        
        return response


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    if 'rag_bot' not in st.session_state:
        index_dir = Path("index")
        if index_dir.exists():
            st.session_state.rag_bot = DreamRAGBot(index_dir)
        else:
            st.error("âŒ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `python scripts/build_index.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            st.stop()


def main():
    """ë©”ì¸ ì•± í•¨ìˆ˜"""
    st.set_page_config(
        page_title="í•œë°¤ì˜ ê¿ˆí•´ëª½ ìƒë‹´ê°€ ğŸŒ™",
        page_icon="ğŸ”®",
        layout="wide"
    )
    
    # í—¤ë”
    st.title("ğŸŒ™ í•œë°¤ì˜ ê¿ˆí•´ëª½ ìƒë‹´ê°€")
    st.markdown("í”„ë¡œì´íŠ¸ì˜ ì •ì‹ ë¶„ì„ê³¼ WHOì˜ ìˆ˜ë©´ê³¼í•™ì´ ë§Œë‚˜ëŠ” ê³³")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("### ğŸ’¡ ì‚¬ìš©ë²•")
        st.markdown("""
        1. ê¿ˆì˜ ë‚´ìš©ì„ ìì„¸íˆ ì…ë ¥í•˜ì„¸ìš”
        2. 3ì´ˆ ì´ë‚´ì— ì„¸ ê°€ì§€ ê´€ì ì˜ í•´ì„ì„ ë°›ì•„ë³´ì„¸ìš”
        3. ë” ë‚˜ì€ ìˆ˜ë©´ì„ ìœ„í•œ ì¡°ì–¸ë„ í•¨ê»˜!
        """)
        
        st.markdown("---")
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        memory = psutil.virtual_memory()
        st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ", f"{memory.percent:.1f}%")
        
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            gc.collect()
            st.rerun()
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    init_session_state()
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar=message.get("avatar")):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì˜¤ëŠ˜ ë°¤ ê¾¼ ê¿ˆì„ ë“¤ë ¤ì£¼ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({
            "role": "user",
            "content": prompt,
            "avatar": "ğŸ§‘"
        })
        
        with st.chat_message("user", avatar="ğŸ§‘"):
            st.markdown(prompt)
        
        # ë´‡ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant", avatar="ğŸ”®"):
            with st.spinner("ê¿ˆì„ í•´ì„í•˜ëŠ” ì¤‘..."):
                start_time = time.time()
                
                # RAG ê²€ìƒ‰
                relevant_chunks = st.session_state.rag_bot.search_similar_chunks(prompt, k=6)
                
                # ì‘ë‹µ ìƒì„±
                response = st.session_state.rag_bot.generate_response(prompt, relevant_chunks)
                
                elapsed_time = time.time() - start_time
                
                # ì‘ë‹µ í‘œì‹œ
                st.markdown(response)
                
                # ì„±ëŠ¥ ì •ë³´ (ì¶•ì†Œ ê°€ëŠ¥)
                with st.expander(f"ğŸ” ë¶„ì„ ì •ë³´ ({elapsed_time:.1f}ì´ˆ)"):
                    st.markdown("**ì°¸ê³  ìë£Œ:**")
                    for i, chunk in enumerate(relevant_chunks[:3], 1):
                        source = chunk['metadata']['source']
                        st.markdown(f"{i}. {source} - ìœ ì‚¬ë„: {chunk['score']:.2f}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
        
        # ë´‡ ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "avatar": "ğŸ”®"
        })


if __name__ == "__main__":
    main() 