"""
Reddit ìƒë‹´ì‚¬ ì±—ë´‡ - RAG ì‹œìŠ¤í…œ
OpenAI API ì‚¬ìš© ë²„ì „ (Streamlit Cloud ë°°í¬ìš©)
TIFUì™€ AITA ë°ì´í„°ë¥¼ í™œìš©í•œ ì¡°ì–¸ ì œê³µ ì„œë¹„ìŠ¤
"""

import os
import gc
import json
import pickle
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import faiss
import numpy as np
import streamlit as st
from sentence_transformers import SentenceTransformer  # í¬ë¡œìŠ¤ í”Œë«í¼ ì§€ì›
from openai import OpenAI

class RedditAdviseBot:
    """Reddit ìƒë‹´ì‚¬ RAG ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self, index_dir: Path):
        """
        ì´ˆê¸°í™”
        
        Args:
            index_dir: ì¸ë±ìŠ¤ íŒŒì¼ ë””ë ‰í† ë¦¬
        """
        self.index_dir = index_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._load_index()
        self._load_embedder()  # í¬ë¡œìŠ¤ í”Œë«í¼ ì„ë² ë” ë¡œë”©
        self._load_llm()
        
    def _load_index(self):
        """FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ íŒ¨ìŠ¤)"""
        index_file = self.index_dir / "reddit_index.faiss"
        chunks_file = self.index_dir / "chunks.pkl"
        config_file = self.index_dir / "config.json"
        
        if not index_file.exists():
            st.info("ğŸ“ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ì—†ì´ ì‘ë™í•©ë‹ˆë‹¤.")
            self.index = None
            self.chunks = []
            self.config = {'total_chunks': 0}
            return
        
        with st.spinner("ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë”©..."):
            try:
                # FAISS ì¸ë±ìŠ¤
                self.index = faiss.read_index(str(index_file))
                
                # ì²­í¬ ë°ì´í„°
                with open(chunks_file, "rb") as f:
                    self.chunks = pickle.load(f)
                
                # ì„¤ì • ì •ë³´
                with open(config_file, "r") as f:
                    self.config = json.load(f)
                
                st.success(f"âœ… {self.config['total_chunks']}ê°œ Reddit í¬ìŠ¤íŠ¸ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                st.warning(f"ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.index = None
                self.chunks = []
                self.config = {'total_chunks': 0}
    
    def _load_embedder(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜)"""
        if self.index is None:
            st.info("ğŸ“ ì¸ë±ìŠ¤ê°€ ì—†ì–´ì„œ ì„ë² ë” ë¡œë”©ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            self.embedder = None
            return
            
        with st.spinner("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©..."):
            try:
                import os
                import platform
                
                # í”Œë«í¼ë³„ ìµœì í™” (ì„ íƒì  ì ìš©)
                if platform.system() == "Darwin" and platform.machine() == "arm64":
                    # Apple Silicon Mac ìµœì í™”
                    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                    os.environ['OMP_NUM_THREADS'] = '1'
                
                # ë²”ìš© ì„¤ì •
                os.environ['TORCH_HOME'] = './models'
                
                # CPU ì‚¬ìš©ìœ¼ë¡œ í¬ë¡œìŠ¤ í”Œë«í¼ ì•ˆì •ì„± í™•ë³´
                import torch
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                self.embedder = SentenceTransformer(
                    self.config['embedding_model'], 
                    device=device,  # ëª¨ë“  í”Œë«í¼ì—ì„œ ì•ˆì •ì 
                    cache_folder="./models"
                )
                self.embedder.max_seq_length = 512  # ì ë‹¹í•œ ê¸¸ì´
                st.success("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                st.warning("ğŸ”„ ê²€ìƒ‰ ì—†ì´ ê¸°ë³¸ ìƒë‹´ ëª¨ë“œë¡œ ì „í™˜")
                self.embedder = None
    
    def _load_llm(self):
        """OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        with st.spinner("ğŸ¤– OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •..."):
            # Streamlit secretsì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            try:
                api_key = st.secrets["OPENAI_API_KEY"]
            except Exception:
                st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.info("Streamlit Cloudì˜ Secretsì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
                raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.client = OpenAI(api_key=api_key)
            st.success("âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ")
    
    def _translate_to_korean(self, text: str) -> str:
        """ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a translator that converts English Reddit posts to natural Korean. Keep the original meaning and tone."},
                    {"role": "user", "content": f"Translate this to Korean: {text}"}
                ],
                max_tokens=500,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text  # ë²ˆì—­ ì‹¤íŒ¨ì‹œ ì›ë¬¸ ë°˜í™˜
    
    def search_similar_chunks(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬í•œ ê²½í—˜ë‹´/ìƒí™© ê²€ìƒ‰ (ì„ì‹œë¡œ ë¹„í™œì„±í™”)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì‚¬ìš©ìì˜ ìƒí™©/ê³ ë¯¼)
            k: ë°˜í™˜í•  ì²­í¬ ìˆ˜
            
        Returns:
            ê´€ë ¨ ê²½í—˜ë‹´ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        """
        # ì¸ë±ìŠ¤ë‚˜ ì„ë² ë”ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜  
        if self.index is None or len(self.chunks) == 0 or not hasattr(self, 'embedder') or self.embedder is None:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ë˜ëŠ” ì„ë² ë”ê°€ ì—†ì–´ì„œ ê²€ìƒ‰ ë¶ˆê°€ - ì¿¼ë¦¬: {query}")
            return []
        
        print(f"ğŸ” ì‹¤ì œ ê²€ìƒ‰ ì‹œì‘ - ì¿¼ë¦¬: {query}")
        
        # ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ í™œì„±í™”!
        expanded_queries = [
            f"ë¹„ìŠ·í•œ ìƒí™© ê²½í—˜ ì¡°ì–¸ {query}",  # í•œêµ­ì–´ ê²€ìƒ‰
            f"similar situation advice experience {query}",  # ì˜ì–´ ê²€ìƒ‰
            f"ë¬¸ì œ í•´ê²° ë„ì›€ {query}",  # ë¬¸ì œ í•´ê²° ê´€ë ¨
            query  # ì›ë³¸ ì¿¼ë¦¬
        ]
        
        all_results = []
        
        # ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ë” í’ë¶€í•œ ê²°ê³¼ í™•ë³´
        for expanded_query in expanded_queries:
            try:
                query_embedding = self.embedder.encode(
                    expanded_query,
                    normalize_embeddings=True,
                    show_progress_bar=False,
                    convert_to_tensor=False  # ì•ˆì •ì„±ì„ ìœ„í•´ numpy ì‚¬ìš©
                )
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ({expanded_query}): {e}")
                continue
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(
                query_embedding.reshape(1, -1).astype('float32'),
                k
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for idx, dist in zip(indices[0], distances[0]):
                if idx != -1:
                    chunk = self.chunks[idx].copy()
                    chunk['score'] = float(1 / (1 + dist))
                    chunk['search_query'] = expanded_query
                    all_results.append(chunk)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_results = {}
        for result in all_results:
            chunk_id = result['metadata']['chunk_id']
            chunk_key = f"{result['metadata']['source']}_{result['metadata']['post_id']}_{chunk_id}"
            if chunk_key not in unique_results or result['score'] > unique_results[chunk_key]['score']:
                unique_results[chunk_key] = result
        
        # ìƒìœ„ kê°œ ë°˜í™˜ (ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©)
        final_results = sorted(unique_results.values(), key=lambda x: x['score'], reverse=True)
        
        # í’ˆì§ˆ í•„í„°ë§: ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ì€ ê²ƒ ì œê±°
        filtered_results = [r for r in final_results if r['score'] > 0.4]
        
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê´€ë ¨ ê²½í—˜ë‹´ ë°œê²¬")
        return filtered_results[:k]
    
    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•´ AITA ìŠ¤íƒ€ì¼ íŒì • ë° ìƒë‹´ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸/ê³ ë¯¼
            context_chunks: ê²€ìƒ‰ëœ ìœ ì‚¬ ê²½í—˜ë‹´
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        from collections import Counter
        
        # íŒì • ì§‘ê³„
        verdicts = []
        verdict_explanations = []
        
        for chunk in context_chunks:
            verdict = chunk['metadata'].get('verdict', 'UNKNOWN')
            if verdict != 'UNKNOWN':
                verdicts.append(verdict)
                # ê°„ë‹¨í•œ ì„¤ëª… ì¶”ê°€
                title_ko = self._translate_to_korean(chunk['metadata'].get('title', ''))
                verdict_explanations.append(f"'{title_ko[:50]}...' â†’ {verdict}")
        
        # íŒì • ê²°ê³¼ ê³„ì‚°
        if verdicts:
            verdict_counts = Counter(verdicts)
            final_verdict = verdict_counts.most_common(1)[0][0]
            verdict_summary = ", ".join([f"{v}: {c}í‘œ" for v, c in verdict_counts.items()])
        else:
            final_verdict = "INFO"
            verdict_summary = "íŒì • ì •ë³´ ë¶€ì¡±"
        
        # íŒì • ì˜ë¯¸ ì„¤ëª…
        verdict_meanings = {
            "YTA": "You're the A-hole (ë‹¹ì‹ ì´ ì˜ëª»í–ˆì–´ìš”)",
            "NTA": "Not the A-hole (ë‹¹ì‹ ì€ ì˜ëª»í•˜ì§€ ì•Šì•˜ì–´ìš”)", 
            "ESH": "Everyone Sucks Here (ëª¨ë‘ê°€ ì˜ëª»í–ˆì–´ìš”)",
            "NAH": "No A-holes Here (ì•„ë¬´ë„ ì˜ëª»í•˜ì§€ ì•Šì•˜ì–´ìš”)",
            "INFO": "Not Enough Info (ì •ë³´ê°€ ë¶€ì¡±í•´ìš”)"
        }
        
        # Reddit ê²½í—˜ë‹´ì„ ë¬¸ë§¥ìœ¼ë¡œ êµ¬ì„± (ë²ˆì—­ í¬í•¨)
        reddit_context = []
        for i, chunk in enumerate(context_chunks, 1):
            metadata = chunk['metadata']
            source = metadata['source']
            title_ko = self._translate_to_korean(metadata.get('title', 'ì œëª© ì—†ìŒ'))
            content_ko = self._translate_to_korean(chunk['text'][:300])  # ê¸´ ë‚´ìš©ì€ ì•ë¶€ë¶„ë§Œ
            score = metadata.get('score', 0)
            verdict = metadata.get('verdict', 'UNKNOWN')
            
            # ìƒìœ„ ëŒ“ê¸€ ì¶”ê°€ (íŒì • ê·¼ê±°ë¡œ í™œìš©)
            comments_text = ""
            comments = metadata.get('comments', [])
            if comments:
                # ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ 2ê°œ ëŒ“ê¸€ë§Œ í¬í•¨
                top_comments = sorted(comments, key=lambda x: x.get('score', 0), reverse=True)[:2]
                for j, comment in enumerate(top_comments, 1):
                    comment_ko = self._translate_to_korean(comment.get('message', '')[:200])
                    comment_score = comment.get('score', 0)
                    comments_text += f"\nìƒìœ„ëŒ“ê¸€{j}: {comment_ko}... (ğŸ‘ {comment_score})"
            
            context_text = f"""
ê²½í—˜ë‹´ {i} [{source}]:
ì œëª©: {title_ko}
ë‚´ìš©: {content_ko}...
Reddit ì ìˆ˜: {score}
ì»¤ë®¤ë‹ˆí‹° íŒì •: {verdict}{comments_text}
"""
            reddit_context.append(context_text.strip())

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (AITA ì¬íŒê´€ ì—­í• )
        system_prompt = f"""ë‹¹ì‹ ì€ Reddit AITA(Am I The A-hole) ì»¤ë®¤ë‹ˆí‹°ì˜ íŒë¡€ë¥¼ í•™ìŠµí•œ **AI ì¬íŒê´€**ì…ë‹ˆë‹¤. ë°°ì„ íŒì‚¬ ì—†ì´ ë‹¨ë…ìœ¼ë¡œ ì‚¬ê±´ì„ ì‹¬ë¦¬í•©ë‹ˆë‹¤.

[ì¬íŒê´€ ì–´íˆ¬ ì§€ì¹¨]
- ì „ë¬¸ ë²•ì¡°ì¸ì˜ ê³µì‹Â·ì—„ìˆ™í•œ ë§íˆ¬ ì‚¬ìš©
- ì²« ë¬¸ë‹¨ì€ ë°˜ë“œì‹œ "ë³¸ ì•ˆê±´ì€ â€¦" í˜•íƒœë¡œ ì‚¬ê±´ ìš”ì§€ë¥¼ ì •ë¦¬
- íŒê²°ë¬¸ ë§ë¯¸ì— **ì£¼ë¬¸** ì„¹ì…˜ ì¶”ê°€: "ì£¼ë¬¸. í”¼ê³ ì¸ì„ YTAë¡œ íŒê²°í•œë‹¤."ì™€ ê°™ì€ í˜•ì‹
- í•„ìš”í•œ ê²½ìš° "íŒì‹œì‚¬í•­", "íŒë‹¨ ê·¼ê±°" í•­ëª©ì„ í¬í•¨
- ë¶ˆí•„ìš”í•œ ê°ì • í‘œí˜„, ë†ë‹´, ìºì£¼ì–¼í•œ í‘œí˜„ ê¸ˆì§€

[íŒê²°ë¬¸ ê¶Œì¥ êµ¬ì¡°]
1. ì„œë¡ : "ë³¸ ì•ˆê±´ì€ â€¦" (ì‚¬ê±´ ìš”ì§€)
2. íŒì‹œì‚¬í•­
3. íŒë‹¨ ê·¼ê±° (ê´€ê³„ ë²•ë ¹Â·íŒë¡€Â·ëŒ“ê¸€ ì¸ìš©)
4. ì£¼ë¬¸ (ìµœì¢… íŒì •: YTA/NTA/ESH/NAH/INFO)
5. í•„ìš” ì‹œ ì¡°ì–¸ ë˜ëŠ” ë¶€ëŒ€ ì˜ê²¬

[íŒì • ê¸°ì¤€]
- YTA (You're the A-hole): ì‚¬ìš©ìì˜ í–‰ë™ì´ ë¶€ì ì ˆí•˜ê±°ë‚˜ ì˜ëª»ë¨
- NTA (Not the A-hole): ì‚¬ìš©ìëŠ” ì˜ëª»í•˜ì§€ ì•ŠìŒ, ìƒëŒ€ë°©ì´ë‚˜ ìƒí™©ì´ ë¬¸ì œ
- ESH (Everyone Sucks Here): ëª¨ë“  ë‹¹ì‚¬ìê°€ ê°ê° ì˜ëª»í•œ ë¶€ë¶„ì´ ìˆìŒ
- NAH (No A-holes Here): ì•„ë¬´ë„ íŠ¹ë³„íˆ ì˜ëª»í•˜ì§€ ì•ŠìŒ, ë‹¨ìˆœí•œ ì˜ê²¬ ì°¨ì´ë‚˜ ë¶ˆí–‰í•œ ìƒí™©
- INFO (Not Enough Info): íŒì •í•˜ê¸°ì— ì •ë³´ê°€ ë¶€ì¡±í•¨
"""

        context_text = "\n\n".join(reddit_context) if reddit_context else "ìœ ì‚¬í•œ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆì§€ë§Œ, ì¼ë°˜ì ì¸ ë„ë•ì  ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        
        user_prompt = f"""ì‚¬ìš©ì ìƒí™©: {query}

ìœ ì‚¬í•œ Reddit AITA íŒë¡€ë“¤:
{context_text}

íŒì • ì§‘ê³„ ê²°ê³¼: {verdict_summary}
ì£¼ìš” íŒì • ê²½í–¥: {final_verdict} ({verdict_meanings.get(final_verdict, final_verdict)})

ìœ„ì˜ íŒë¡€ë“¤ê³¼ ì»¤ë®¤ë‹ˆí‹° ì˜ê²¬ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ìƒí™©ì— ëŒ€í•´ ê³µì •í•œ AITA íŒì •ì„ ë‚´ë ¤ì£¼ì„¸ìš”."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def chat(self, user_input: str) -> Tuple[str, List[Dict]]:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ (ê³ ë¯¼/ìƒí™©)
            
        Returns:
            Tuple[ì‘ë‹µ í…ìŠ¤íŠ¸, ì°¸ê³ í•œ ê²½í—˜ë‹´ ëª©ë¡]
        """
        with st.spinner("ğŸ” ë¹„ìŠ·í•œ ê²½í—˜ë‹´ì„ ì°¾ê³  ìˆì–´ìš”..."):
            # ìœ ì‚¬í•œ ê²½í—˜ë‹´ ê²€ìƒ‰
            similar_chunks = self.search_similar_chunks(user_input, k=3)
            
        with st.spinner("ğŸ’­ ì¡°ì–¸ì„ ì¤€ë¹„í•˜ê³  ìˆì–´ìš”..."):
            # ì‘ë‹µ ìƒì„±
            response = self.generate_response(user_input, similar_chunks)
            
        return response, similar_chunks


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "bot" not in st.session_state:
        # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ í™•ì¸ (ì„ì‹œë¡œ ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬)
        index_dir = Path("index")
        if not index_dir.exists() or not (index_dir / "reddit_index.faiss").exists():
            st.warning("âš ï¸ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ì—†ì´ ê¸°ë³¸ ìƒë‹´ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
            st.info("ì™„ì „í•œ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” `python scripts/build_index.py`ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            # ì„ì‹œ ë”ë¯¸ ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
            index_dir.mkdir(exist_ok=True)
        
        try:
            # ë´‡ ì´ˆê¸°í™” (ì¸ë±ìŠ¤ ì—†ì–´ë„ ì‘ë™í•˜ë„ë¡)
            st.session_state.bot = RedditAdviseBot(index_dir)
        except Exception as e:
            st.error(f"ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            st.info("ì¸ë±ìŠ¤ê°€ ì—†ì–´ë„ ê¸°ë³¸ ìƒë‹´ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            st.session_state.bot = None


def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    st.set_page_config(
        page_title="Reddit ìƒë‹´ì‚¬ ğŸ¤—",
        page_icon="ğŸ¤—",
        layout="wide"
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    init_session_state()
    
    # í—¤ë”
    st.title("âš–ï¸ AITA ì¬íŒë¶€")
    st.markdown("""
    ğŸ“œ **ë³¸ ë²•ì •ì€ Reddit AITA ì»¤ë®¤ë‹ˆí‹°ì˜ ë°©ëŒ€í•œ íŒë¡€ë¥¼ í•™ìŠµí•œ AI ì¬íŒê´€ì…ë‹ˆë‹¤.**  
    ì‚¬ê±´ì˜ ì‚¬ì‹¤ê´€ê³„ë¥¼ ì§„ìˆ í•˜ì‹œë©´, ì„ ë¡€ì™€ ëŒ“ê¸€ì„ ê·¼ê±°ë¡œ **YTA/NTA/ESH/NAH** ì¤‘ í•˜ë‚˜ì˜ íŒê²°ì„ ì„ ê³ í•˜ê² ìŠµë‹ˆë‹¤.
    """)
    
    # AITA ì•½ì–´ ì„¤ëª…
    with st.expander("ğŸ“– AITA íŒì • ê¸°ì¤€", expanded=False):
        st.markdown("""
        **YTA** (You're the A-hole) - ë‹¹ì‹ ì´ ì˜ëª»í–ˆì–´ìš”  
        **NTA** (Not the A-hole) - ë‹¹ì‹ ì€ ì˜ëª»í•˜ì§€ ì•Šì•˜ì–´ìš”  
        **ESH** (Everyone Sucks Here) - ëª¨ë‘ê°€ ì˜ëª»í–ˆì–´ìš”  
        **NAH** (No A-holes Here) - ì•„ë¬´ë„ ì˜ëª»í•˜ì§€ ì•Šì•˜ì–´ìš”  
        **INFO** (Not Enough Info) - ì •ë³´ê°€ ë¶€ì¡±í•´ìš”  
        """)
    
    # ì‚¬ì´ë“œë°” - ì‚¬ìš©ë²• ì•ˆë‚´
    with st.sidebar:
        st.header("ğŸ“– ì‚¬ìš©ë²•")
        st.markdown("""
        **ì–´ë–¤ íŒì •ì„ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?**
        - ì¸ê°„ê´€ê³„ ê°ˆë“± ìƒí™©
        - ë„ë•ì /ìœ¤ë¦¬ì  ë”œë ˆë§ˆ
        - ì¼ìƒìƒí™œì—ì„œì˜ ì„ íƒê³¼ í–‰ë™
        - ê°€ì¡±, ì¹œêµ¬, ì—°ì¸ê³¼ì˜ ë¬¸ì œ
        - ì§ì¥ì´ë‚˜ í•™êµì—ì„œì˜ ê°ˆë“±
        
        **ì˜ˆì‹œ ìƒí™©:**
        - "ì¹œêµ¬ ê²°í˜¼ì‹ì— ëª» ê°„ë‹¤ê³  í–ˆëŠ”ë°..."
        - "ë£¸ë©”ì´íŠ¸ê°€ ì²­ì†Œë¥¼ ì•ˆ í•´ì„œ í™”ëƒˆì–´ìš”"
        - "ë¶€ëª¨ë‹˜ì´ ì›í•˜ì§€ ì•ŠëŠ” ì„ íƒì„ í–ˆì–´ìš”"
        - "ë‚¨ìì¹œêµ¬ì™€ ëˆ ë¬¸ì œë¡œ ì‹¸ì› ì–´ìš”"
        """)
        
        st.header("âš–ï¸ íŒì • ë°©ì‹")
        st.markdown("""
        - Reddit AITA ì»¤ë®¤ë‹ˆí‹° íŒë¡€ ë¶„ì„
        - ë¹„ìŠ·í•œ ìƒí™©ì˜ ì§‘ë‹¨ ì§€ì„± í™œìš©
        - ê³µì •í•˜ê³  ê°ê´€ì ì¸ ë„ë•ì  íŒë‹¨
        - ê±´ì„¤ì ì¸ í•´ê²°ì±… ì œì‹œ
        """)
        
        st.header("âš ï¸ ì£¼ì˜ì‚¬í•­")
        st.markdown("""
        - ì¬ë¯¸ì™€ ì„±ì°°ì„ ìœ„í•œ íŒì •ì…ë‹ˆë‹¤
        - ì „ë¬¸ì ì¸ ìƒë‹´ì€ ì „ë¬¸ê°€ì—ê²Œ
        - ê°œì¸ì •ë³´ëŠ” ì…ë ¥í•˜ì§€ ë§ˆì„¸ìš”
        - íŒì •ì— ë„ˆë¬´ ì˜ì¡´í•˜ì§€ ë§ˆì„¸ìš”
        """)
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
            
            # ì°¸ê³  ê²½í—˜ë‹´ í‘œì‹œ (assistant ë©”ì‹œì§€ì—ë§Œ)
            if message["role"] == "assistant" and "references" in message:
                references = message["references"]
                
                # íŒì • ì§‘ê³„ í‘œì‹œ
                from collections import Counter
                verdicts = [ref['metadata'].get('verdict', 'UNKNOWN') for ref in references if ref['metadata'].get('verdict', 'UNKNOWN') != 'UNKNOWN']
                
                if verdicts:
                    verdict_counts = Counter(verdicts)
                    st.markdown("### âš–ï¸ AITA ì»¤ë®¤ë‹ˆí‹° íŒì • ì§‘ê³„")
                    
                    # íŒì • ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ í‘œì‹œ
                    cols = st.columns(len(verdict_counts))
                    for i, (verdict, count) in enumerate(verdict_counts.items()):
                        with cols[i]:
                            st.metric(verdict, f"{count}í‘œ")
                
                with st.expander("ğŸ“š ì°¸ê³ í•œ ê²½í—˜ë‹´ë“¤ (í•œêµ­ì–´ ë²ˆì—­)", expanded=False):
                    for i, ref in enumerate(references, 1):
                        source = ref['metadata']['source']
                        title = ref['metadata'].get('title', 'ì œëª© ì—†ìŒ')
                        score = ref.get('score', 0)
                        verdict = ref['metadata'].get('verdict', 'UNKNOWN')
                        reddit_score = ref['metadata'].get('score', 0)
                        url = ref['metadata'].get('url', '')
                        
                        # ì œëª©ê³¼ ë‚´ìš© ë²ˆì—­ (ë´‡ì´ ìˆì„ ë•Œë§Œ)
                        if st.session_state.bot and hasattr(st.session_state.bot, '_translate_to_korean'):
                            title_ko = st.session_state.bot._translate_to_korean(title)
                            content_ko = st.session_state.bot._translate_to_korean(ref['text'][:300])
                        else:
                            title_ko = title
                            content_ko = ref['text'][:300]
                        
                        # ë§í¬ í¬í•¨ ì œëª©
                        if url:
                            title_display = f"[{title_ko}]({url})"
                        else:
                            title_display = title_ko
                        
                        st.markdown(f"""
                        **{i}. [{source}] {title_display}**  
                        ìœ ì‚¬ë„: {score:.3f} | Reddit ì ìˆ˜: {reddit_score} | ì»¤ë®¤ë‹ˆí‹° íŒì •: **{verdict}**  
                        
                        {content_ko}...
                        """)
                        
                        # ìƒìœ„ ëŒ“ê¸€ í‘œì‹œ
                        comments = ref['metadata'].get('comments', [])
                        if comments:
                            st.markdown("**ğŸ’¬ ì£¼ìš” ëŒ“ê¸€ë“¤:**")
                            top_comments = sorted(comments, key=lambda x: x.get('score', 0), reverse=True)[:3]
                            for j, comment in enumerate(top_comments, 1):
                                if st.session_state.bot and hasattr(st.session_state.bot, '_translate_to_korean'):
                                    comment_ko = st.session_state.bot._translate_to_korean(comment.get('message', '')[:250])
                                else:
                                    comment_ko = comment.get('message', '')[:250]
                                comment_score = comment.get('score', 0)
                                st.markdown(f"- **ëŒ“ê¸€{j}:** {comment_ko}... _(ğŸ‘ {comment_score})_")
                        
                        st.markdown("---")
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("íŒì •ë°›ê³  ì‹¶ì€ ìƒí™©ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”... (ì˜ˆ: ë‚´ê°€ ì˜ëª»í•œ ê±¸ê¹Œìš”?)"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # ë´‡ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            try:
                if st.session_state.bot is None:
                    # ë´‡ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
                    response = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ë¬¸ì œê°€ ìˆì–´ ì œí•œëœ ê¸°ëŠ¥ë§Œ ì œê³µë©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ìƒë‹´ ì¡°ì–¸ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤ë§Œ, ì™„ì „í•œ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                    references = []
                else:
                    response, references = st.session_state.bot.chat(prompt)
                
                st.write(response)
                
                # ì°¸ê³  ê²½í—˜ë‹´ í‘œì‹œ
                if references:
                    # íŒì • ì§‘ê³„ í‘œì‹œ
                    from collections import Counter
                    verdicts = [ref['metadata'].get('verdict', 'UNKNOWN') for ref in references if ref['metadata'].get('verdict', 'UNKNOWN') != 'UNKNOWN']
                    
                    if verdicts:
                        verdict_counts = Counter(verdicts)
                        st.markdown("### âš–ï¸ AITA ì»¤ë®¤ë‹ˆí‹° íŒì • ì§‘ê³„")
                        
                        # íŒì • ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ í‘œì‹œ
                        cols = st.columns(len(verdict_counts))
                        for i, (verdict, count) in enumerate(verdict_counts.items()):
                            with cols[i]:
                                st.metric(verdict, f"{count}í‘œ")
                    
                    with st.expander("ğŸ“š ì°¸ê³ í•œ ê²½í—˜ë‹´ë“¤ (í•œêµ­ì–´ ë²ˆì—­)", expanded=False):
                        for i, ref in enumerate(references, 1):
                            source = ref['metadata']['source']
                            title = ref['metadata'].get('title', 'ì œëª© ì—†ìŒ')
                            score = ref.get('score', 0)
                            verdict = ref['metadata'].get('verdict', 'UNKNOWN')
                            reddit_score = ref['metadata'].get('score', 0)
                            url = ref['metadata'].get('url', '')
                            
                            # ì œëª©ê³¼ ë‚´ìš© ë²ˆì—­ (ì‹¤ì‹œê°„)
                            if hasattr(st.session_state.bot, '_translate_to_korean'):
                                title_ko = st.session_state.bot._translate_to_korean(title)
                                content_ko = st.session_state.bot._translate_to_korean(ref['text'][:300])
                            else:
                                title_ko = title
                                content_ko = ref['text'][:300]
                            
                            # ë§í¬ í¬í•¨ ì œëª©
                            if url:
                                title_display = f"[{title_ko}]({url})"
                            else:
                                title_display = title_ko
                            
                            st.markdown(f"""
                            **{i}. [{source}] {title_display}**  
                            ìœ ì‚¬ë„: {score:.3f} | Reddit ì ìˆ˜: {reddit_score} | ì»¤ë®¤ë‹ˆí‹° íŒì •: **{verdict}**  
                            
                            {content_ko}...
                            """)
                            
                            # ìƒìœ„ ëŒ“ê¸€ í‘œì‹œ
                            comments = ref['metadata'].get('comments', [])
                            if comments:
                                st.markdown("**ğŸ’¬ ì£¼ìš” ëŒ“ê¸€ë“¤:**")
                                top_comments = sorted(comments, key=lambda x: x.get('score', 0), reverse=True)[:3]
                                for j, comment in enumerate(top_comments, 1):
                                    if hasattr(st.session_state.bot, '_translate_to_korean'):
                                        comment_ko = st.session_state.bot._translate_to_korean(comment.get('message', '')[:250])
                                    else:
                                        comment_ko = comment.get('message', '')[:250]
                                    comment_score = comment.get('score', 0)
                                    st.markdown(f"- **ëŒ“ê¸€{j}:** {comment_ko}... _(ğŸ‘ {comment_score})_")
                            
                            st.markdown("---")
                
                # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "references": references
                })
                
            except Exception as e:
                error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": error_msg
                })
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()


if __name__ == "__main__":
    main() 